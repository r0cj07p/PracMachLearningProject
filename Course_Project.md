# Building a Machine Learning Algorithm to Predict the Manner of Exercise from Accelerometer Data
Dave Moody  
Thursday, May 12, 2016  
#Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data 
about personal activity relatively inexpensively. These type of devices are part of the quantified self movement, 
a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns 
in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a 
particular activity they do, but they rarely quantify how well they do it. 

For this project, 6 participants were asked to perform barbell lifts correctly and incorrectly in 5 different
ways, generating data from accelerometers positioned on the belt, forearm, arm, and dumbell of the participants. 

Using this data, the goal is to build a machine learning algorithm that predicts the manner in which the
participants performed an exercise, based on the measurements generated by the accelerometers during the exercise
.

#The Data
The data consists of 19642 observations of 160 different variables. Each observation is the execution of
a barbell lift by one of the participants.  The dataset is already divided into a training set and a testing 
set, with 20 of the observations allocated to the testing set to test the predictive algorithm at the conclusion 
of the predictive model building process.  

The outcome variable which the model will predict is the "classe" variable.  It is a factor variable with 5 
levels ("A", "B", "C", "D" and "E").  An "A" in the classe column indicates that the lift was performed correctly
, as determined by an experienced weight-lifter who supervised and directed the participants when performing the 
lifts.  Each lift was directed by the supervisor to be performed either correctly or with one or more specific 
movement errors (the participants used light weights to minimize or negate the risk of injury). Lifts performed 
with incorrect movements were assigned values "B" through "E" in the classe column, depending on the specific 
type of movement error performed.

The remaining variables in the dataset consist of the various measurement data collected by the accelerometers on 
the participants while they performed the lifts.  

The data, plus more information on Human Activity Recognition have been made available from the following website
: 
http://groupware.les.inf.puc-rio.br/har

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data 
Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence
. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, 
PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.

#Loading and Partitioning the Data
After downloading the data from the website above, the training dataset is read into Rstudio and partitioned into 
sub-training and testing sets.  The 20-observation testing set provided by the website will be used later as a 
validation set, and not loaded at this time or during any portion of the model building or testing phases. The 
following code assumes that the working directory is set to the file path containing the downloaded training set,
and that the name of the file was not changed.

```r
library(caret)
```

```
## Warning: package 'caret' was built under R version 3.2.5
```

```
## Warning: package 'ggplot2' was built under R version 3.2.4
```

```r
lifting <- read.csv("pml-training.csv")
set.seed(1234)
inTrain <- createDataPartition(y=lifting$classe, p=0.6, list=FALSE)
training <- lifting[inTrain,]
testing <- lifting[-inTrain,]
```
Sixty percent of the observations in the lifting dataset are now allocated to a training set, and 40% are 
allocated to a testing set.  

#Preparing Data for Analysis
Prior to any model building, we will prepare the data for the analysis functions to follow. Missing values may 
need to be imputed, or some variables may need to be recategorized, combined or removed altogether depending on
their usefulness.  

###Removing Unnecessary Variables
If a variable in our datasets have zero or near-zero variability, it will serve no purpose in an analysis and 
should be removed from the datasets:

```r
nsv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nsv$nzv=="FALSE"]
testing <- testing[,nsv$nzv=="FALSE"]
```
This operation has trimmed the number of variables down from 160 to 105. 

A review of the remaining variables in the training set reveals that there are several columns where most of 
the values in the column are missing (NAs).  Interestingly, the columns identified as containing NAs appear to 
all contain the exact same number of NAs (11540 per column in the training set, constituting 97.9% of the 
observations, and 7676 per column in the testing set, also constituting 97.9% of the observations).  With only  
2.1% of the total observations in these columns containing data, these variables will be eliminated from the 
training and testing sets, as there are not enough values present in these columns to reasonably impute the 
missing ones, and these variables would have no predictive value in a model. 

```r
colSums(is.na(training))
```

```
##                        X                user_name     raw_timestamp_part_1 
##                        0                        0                        0 
##     raw_timestamp_part_2           cvtd_timestamp               num_window 
##                        0                        0                        0 
##                roll_belt               pitch_belt                 yaw_belt 
##                        0                        0                        0 
##         total_accel_belt            max_roll_belt           max_picth_belt 
##                        0                    11540                    11540 
##            min_roll_belt           min_pitch_belt      amplitude_roll_belt 
##                    11540                    11540                    11540 
##     amplitude_pitch_belt     var_total_accel_belt            avg_roll_belt 
##                    11540                    11540                    11540 
##         stddev_roll_belt            var_roll_belt           avg_pitch_belt 
##                    11540                    11540                    11540 
##        stddev_pitch_belt           var_pitch_belt             avg_yaw_belt 
##                    11540                    11540                    11540 
##          stddev_yaw_belt             var_yaw_belt             gyros_belt_x 
##                    11540                    11540                        0 
##             gyros_belt_y             gyros_belt_z             accel_belt_x 
##                        0                        0                        0 
##             accel_belt_y             accel_belt_z            magnet_belt_x 
##                        0                        0                        0 
##            magnet_belt_y            magnet_belt_z                 roll_arm 
##                        0                        0                        0 
##                pitch_arm                  yaw_arm          total_accel_arm 
##                        0                        0                        0 
##            var_accel_arm              gyros_arm_x              gyros_arm_y 
##                    11540                        0                        0 
##              gyros_arm_z              accel_arm_x              accel_arm_y 
##                        0                        0                        0 
##              accel_arm_z             magnet_arm_x             magnet_arm_y 
##                        0                        0                        0 
##             magnet_arm_z            max_picth_arm              max_yaw_arm 
##                        0                    11540                    11540 
##            min_pitch_arm              min_yaw_arm      amplitude_pitch_arm 
##                    11540                    11540                    11540 
##        amplitude_yaw_arm            roll_dumbbell           pitch_dumbbell 
##                    11540                        0                        0 
##             yaw_dumbbell        max_roll_dumbbell       max_picth_dumbbell 
##                        0                    11540                    11540 
##        min_roll_dumbbell       min_pitch_dumbbell  amplitude_roll_dumbbell 
##                    11540                    11540                    11540 
## amplitude_pitch_dumbbell     total_accel_dumbbell       var_accel_dumbbell 
##                    11540                        0                    11540 
##        avg_roll_dumbbell     stddev_roll_dumbbell        var_roll_dumbbell 
##                    11540                    11540                    11540 
##       avg_pitch_dumbbell    stddev_pitch_dumbbell       var_pitch_dumbbell 
##                    11540                    11540                    11540 
##         avg_yaw_dumbbell      stddev_yaw_dumbbell         var_yaw_dumbbell 
##                    11540                    11540                    11540 
##         gyros_dumbbell_x         gyros_dumbbell_y         gyros_dumbbell_z 
##                        0                        0                        0 
##         accel_dumbbell_x         accel_dumbbell_y         accel_dumbbell_z 
##                        0                        0                        0 
##        magnet_dumbbell_x        magnet_dumbbell_y        magnet_dumbbell_z 
##                        0                        0                        0 
##             roll_forearm            pitch_forearm              yaw_forearm 
##                        0                        0                        0 
##         max_roll_forearm        max_picth_forearm         min_roll_forearm 
##                    11540                    11540                    11540 
##        min_pitch_forearm   amplitude_roll_forearm  amplitude_pitch_forearm 
##                    11540                    11540                    11540 
##      total_accel_forearm        var_accel_forearm          gyros_forearm_x 
##                        0                    11540                        0 
##          gyros_forearm_y          gyros_forearm_z          accel_forearm_x 
##                        0                        0                        0 
##          accel_forearm_y          accel_forearm_z         magnet_forearm_x 
##                        0                        0                        0 
##         magnet_forearm_y         magnet_forearm_z                   classe 
##                        0                        0                        0
```

```r
colSums(is.na(testing))
```

```
##                        X                user_name     raw_timestamp_part_1 
##                        0                        0                        0 
##     raw_timestamp_part_2           cvtd_timestamp               num_window 
##                        0                        0                        0 
##                roll_belt               pitch_belt                 yaw_belt 
##                        0                        0                        0 
##         total_accel_belt            max_roll_belt           max_picth_belt 
##                        0                     7676                     7676 
##            min_roll_belt           min_pitch_belt      amplitude_roll_belt 
##                     7676                     7676                     7676 
##     amplitude_pitch_belt     var_total_accel_belt            avg_roll_belt 
##                     7676                     7676                     7676 
##         stddev_roll_belt            var_roll_belt           avg_pitch_belt 
##                     7676                     7676                     7676 
##        stddev_pitch_belt           var_pitch_belt             avg_yaw_belt 
##                     7676                     7676                     7676 
##          stddev_yaw_belt             var_yaw_belt             gyros_belt_x 
##                     7676                     7676                        0 
##             gyros_belt_y             gyros_belt_z             accel_belt_x 
##                        0                        0                        0 
##             accel_belt_y             accel_belt_z            magnet_belt_x 
##                        0                        0                        0 
##            magnet_belt_y            magnet_belt_z                 roll_arm 
##                        0                        0                        0 
##                pitch_arm                  yaw_arm          total_accel_arm 
##                        0                        0                        0 
##            var_accel_arm              gyros_arm_x              gyros_arm_y 
##                     7676                        0                        0 
##              gyros_arm_z              accel_arm_x              accel_arm_y 
##                        0                        0                        0 
##              accel_arm_z             magnet_arm_x             magnet_arm_y 
##                        0                        0                        0 
##             magnet_arm_z            max_picth_arm              max_yaw_arm 
##                        0                     7676                     7676 
##            min_pitch_arm              min_yaw_arm      amplitude_pitch_arm 
##                     7676                     7676                     7676 
##        amplitude_yaw_arm            roll_dumbbell           pitch_dumbbell 
##                     7676                        0                        0 
##             yaw_dumbbell        max_roll_dumbbell       max_picth_dumbbell 
##                        0                     7676                     7676 
##        min_roll_dumbbell       min_pitch_dumbbell  amplitude_roll_dumbbell 
##                     7676                     7676                     7676 
## amplitude_pitch_dumbbell     total_accel_dumbbell       var_accel_dumbbell 
##                     7676                        0                     7676 
##        avg_roll_dumbbell     stddev_roll_dumbbell        var_roll_dumbbell 
##                     7676                     7676                     7676 
##       avg_pitch_dumbbell    stddev_pitch_dumbbell       var_pitch_dumbbell 
##                     7676                     7676                     7676 
##         avg_yaw_dumbbell      stddev_yaw_dumbbell         var_yaw_dumbbell 
##                     7676                     7676                     7676 
##         gyros_dumbbell_x         gyros_dumbbell_y         gyros_dumbbell_z 
##                        0                        0                        0 
##         accel_dumbbell_x         accel_dumbbell_y         accel_dumbbell_z 
##                        0                        0                        0 
##        magnet_dumbbell_x        magnet_dumbbell_y        magnet_dumbbell_z 
##                        0                        0                        0 
##             roll_forearm            pitch_forearm              yaw_forearm 
##                        0                        0                        0 
##         max_roll_forearm        max_picth_forearm         min_roll_forearm 
##                     7676                     7676                     7676 
##        min_pitch_forearm   amplitude_roll_forearm  amplitude_pitch_forearm 
##                     7676                     7676                     7676 
##      total_accel_forearm        var_accel_forearm          gyros_forearm_x 
##                        0                     7676                        0 
##          gyros_forearm_y          gyros_forearm_z          accel_forearm_x 
##                        0                        0                        0 
##          accel_forearm_y          accel_forearm_z         magnet_forearm_x 
##                        0                        0                        0 
##         magnet_forearm_y         magnet_forearm_z                   classe 
##                        0                        0                        0
```

```r
training <- training[colSums(is.na(training)) < 11539]
testing <- testing[colSums(is.na(testing)) < 7675]
```
This operation has further trimmed the number of variables down from 105 to 59. According to the output of the 
"colSums" commands above, the remaining variables contain no missing values.

A review of the remaining variables reveals a small number of columns containing data that are not pertinent 
to our outcome variable of interest.  These include the data contained in the first 6 columns, which are 
observation/participant identifiers and timestamp data.  These will be removed manually.

```r
training <- training[,7:59]
testing <- testing[,7:59]
```
This operation has trimmed the number of remaining variables to 53. 

The final preprocessing step will be to examine the data for variables that are highly correlated with each 
other,and combine variables if necessary. We set a correlation coefficient threshold of .9, and identify the 
variables with correlation coefficients of .9 or higher:

```r
M <- abs(cor(training[,-53]))
diag(M) <- 0  #negate the instances of variables being perfectly correlated with themselves by changing their value to 0 
which(M > .9, arr.ind=T)
```

```
##                  row col
## total_accel_belt   4   1
## accel_belt_y       9   1
## accel_belt_z      10   1
## accel_belt_x       8   2
## roll_belt          1   4
## accel_belt_y       9   4
## accel_belt_z      10   4
## pitch_belt         2   8
## roll_belt          1   9
## total_accel_belt   4   9
## accel_belt_z      10   9
## roll_belt          1  10
## total_accel_belt   4  10
## accel_belt_y       9  10
## gyros_arm_y       19  18
## gyros_arm_x       18  19
## gyros_dumbbell_z  33  31
## gyros_forearm_z   46  31
## gyros_dumbbell_x  31  33
## gyros_forearm_z   46  33
## gyros_dumbbell_x  31  46
## gyros_dumbbell_z  33  46
```
###Combining Variables?
The output above shows that several variables have a very high correleation with 2-3 others.  This list is even 
longer when the correlation coefficient threshold is lowered to .8, which is still a high correlation.  We may 
want to combine these highly correlated variables into single multivariables (via a principal component analysis) 
prior to building our predictive model, as this may effectively reduce the number of variables that the model 
has to consider while explaining the largest amount of variance in the data.  This could result in a more 
accurate model, but depending on how many multivariables we need in order to account for a large chunk of the 
variance, we may still end up with a large number of variables, which would defeat the purpose.  

To test this, we'll analyze how many principal components (PCs) we would need to account for 80% of the variance 
in the data:

```r
set.seed(1234)
preObj80 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.8)
preObj80
```

```
## Created from 11776 samples and 52 variables
## 
## Pre-processing:
##   - centered (52)
##   - ignored (0)
##   - principal component signal extraction (52)
##   - scaled (52)
## 
## PCA needed 12 components to capture 80 percent of the variance
```
According to the output above, we would need to create 12 PCs to account for 80% of the variance in the data.  
This indicates that it's not worthwhile to preprocess our variables in this manner, as it will hinder 
their interpretability by combining, centering and scaling the original variables into unrecognizable 
multivariables, and in the end, we would still be passing a lot of variables to the predictive models anyway, and 
risk overfitting the model.    
 
##Building a Predictive Model
Based on what was learned in the above review and cleaning of the dataset, a random forest method is being 
selected to build the predictive model.  The rationale for selecting this method is because it is well-suited to 
the classes of the predictors in our data (numeric and integer) and the outcome (factor). Random forests 
utililize classification trees to make predictions by placing cut points on numeric or integer data (which are
determined by analyzing the training data), then assigning the observations to different outcome nodes based on 
whether the value of the predictor variable is greater than or less than cut point. The variables that make the 
best predictors are selected by the model based on which ones produce the most homogenous groups, which also 
makes this method preferable since there are several dozen predictor variables in a detaset.

The random forest method also utilizes a built-in cross validation step in the form of resampling of both the 
observed data and the predictor variables at each level of the tree.  This way, each time the node of a tree is 
being evaluated, the method allows for a different subset of the predictor variables to potentially contribute to 
the next split, which is important since we have so many variables possibly contributing to 1 of 5 different 
outcomes in this case.

```r
set.seed(1234)
library(randomForest)
```

```
## Warning: package 'randomForest' was built under R version 3.2.5
```

```r
modFit_rf <- randomForest(classe ~ .,data=training, importance=TRUE)
modFit_rf
```

```
## 
## Call:
##  randomForest(formula = classe ~ ., data = training, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 0.66%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 3346    2    0    0    0 0.0005973716
## B   18 2257    4    0    0 0.0096533567
## C    0   17 2034    3    0 0.0097370983
## D    0    0   23 1905    2 0.0129533679
## E    0    0    3    6 2156 0.0041570439
```
The output of this model fit shows a very good performance when applied to the training data, according to the 
confusion matrix.  

Below is a table showing the 6 most important variables for determining the outcome by the model.  The variable 
importance measure here is based on weighted sums of the absolute regression coefficients. The weights are a 
function of the reduction of the sums of squares across the number of partial least squares components and are 
computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally 
to the reduction in the sums of squares. A "Max" column was added which identifies the highest weight value each
variable contributed, and the matrix is then ordered by the highest "Max" weight value, so looking at the top 6 rows of the newly ordered matrix shows the 6 variables with the most importance in determining outcome:

```r
imp <- varImp(modFit_rf)
imp$Max <- apply(imp, 1, max)
impOrder <- imp[order(-imp$Max),]
head(impOrder)
```

```
##                          A        B        C        D        E      Max
## pitch_belt        30.07643 47.48016 36.50681 32.79330 29.61868 47.48016
## yaw_belt          47.06115 42.74176 41.43248 46.83199 30.04672 47.06115
## roll_belt         38.33813 45.19152 42.73724 43.54046 40.10237 45.19152
## magnet_dumbbell_z 42.22417 36.85924 44.69710 34.62175 33.08330 44.69710
## magnet_dumbbell_y 34.16786 35.32123 44.17598 36.68571 32.72598 44.17598
## pitch_forearm     31.92073 34.22113 36.59036 34.31207 32.16853 36.59036
```
#Estimating the Out of Sample Error
With the random forest method, there is no need for cross-validation or a separate test set to get an unbiased 
estimate of the test set error. It is estimated internally, during the run, and displayed in the output when 
the model is called.  For this model, the out of sample error rate estimate is only 0.66%, meaning we should 
expect this model to be > 99% accurate when applied to a distinct set of observations reserved for testing.

#Testing the Model on the Testing Sample
Now the random forest model we built will be applied to the testing data to predict outcomes on a new sample:

```r
set.seed(1234)
predictionsTestSet <- predict(modFit_rf,testing)
confusionMatrix(predictionsTestSet, testing$classe)
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2231   12    0    0    0
##          B    0 1501   12    0    0
##          C    0    5 1353   17    2
##          D    1    0    3 1267    2
##          E    0    0    0    2 1438
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9929          
##                  95% CI : (0.9907, 0.9946)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.991           
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9996   0.9888   0.9890   0.9852   0.9972
## Specificity            0.9979   0.9981   0.9963   0.9991   0.9997
## Pos Pred Value         0.9947   0.9921   0.9826   0.9953   0.9986
## Neg Pred Value         0.9998   0.9973   0.9977   0.9971   0.9994
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2843   0.1913   0.1724   0.1615   0.1833
## Detection Prevalence   0.2859   0.1928   0.1755   0.1622   0.1835
## Balanced Accuracy      0.9987   0.9935   0.9927   0.9922   0.9985
```
According to the confusion matrix above, the model predicted the outcomes in the testing data with over 99.2%
accuracy, with a 95% confidence interval of 99.07% on the low end.  With such a highly accurate result, this 
model is ready to be applied to the validation set.  The outcomes will be used to answer the quiz questions 
associated with this project.  

#Applying the model to the Validation Set
First, we have to load the validation dataset we downloaded previously into R, and clean it in the same manner as 
the training and test sets so that the predictor variables in each dataset are the same:

```r
ValSet <- read.csv("pml-testing.csv")
TrainColumns <- colnames(training[,-53]) #get the column names used in the training set
ValSet <- ValSet[TrainColumns] #subset the validation set, keeping only the same columns as the training set
```
Now we apply the random forest model to the validation set to predict the outcomes:

```r
set.seed(1234)
predictionsValSet <- predict(modFit_rf,ValSet)
predictionsValSet
```

```
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
```
The predictions above will now be submitted to complete the quiz.  
