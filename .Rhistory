#Measures of impurity:
#Deviance/information gain:
#-\sum_{k=1}^K \hat{p}_{mk} \log_2\hat{p}_{mk} $$
#0 = perfect purity
#1 = no purity
#Example:
data(iris); library(ggplot2)
names(iris)
#Output:  "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"  "Species"
table(iris$Species)
#Output:  setosa versicolor  virginica
#             50         50         50
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE) #<- partition data at 70% training/testing on the outcome variable "species"
training <- iris[inTrain,]  #<- name the training and test sets.
testing <- iris[-inTrain,]
qplot(Petal.Width,Sepal.Width,colour=Species,data=training) #<- scatterplot of petal width on X axis and sepal width on Y axis, with data points colored by the species variable
#scatterplot shows 3 distinct clusters in the data accounted for by the "species" variable
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training) #<- Fit a model using the training data with the outcome "species" and all other variables as predictors, using the "rpart" method, which is R's package for doing regression and classification trees
print(modFit$finalModel)
#output:
#n= 105  <- number of observations in training set
#node), split, n, loss, yval, (yprob)
#* denotes terminal node
#1) root 105 70 setosa (0.3333333 0.3333333 0.3333333)
#2) Petal.Length< 2.6 35  0 setosa (1.0000000 0.0000000 0.0000000) *  <- All petal lengths less than 2.6 belong to species "setosa"
#        3) Petal.Length>=2.6 70 35 versicolor (0.0000000 0.5000000 0.5000000)  <- Petal lengths greater than 2.6 have an equal change of being in either of the 2 remaining species
#6) Petal.Width< 1.75 39  4 versicolor (0.0000000 0.8974359 0.1025641) *  <- Petal widths less than 1.75 have a 90% chance of being in the versicolor species
#        7) Petal.Width>=1.75 31  0 virginica (0.0000000 0.0000000 1.0000000) * <- Petal widths greater than or equal to 1.75 are all in the virginica species
#Plot of the tree:
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
#A better plot can be made with the "rattle" package:
library(rattle)
fancyRpartPlot(modFit$finalModel)
data(iris); library(ggplot2)
names(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
install.packages("ElemStatLearn")
library(ElemStatLearn); data(ozone,package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
dim(ozone)
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
ll <- matrix(NA,nrow=10,ncol=155)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
for(i in 1:10){
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
library(caret)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
install.packages("party")
library(party)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = ctreeBag$fit,
predict = ctreeBag$pred,
aggregate = ctreeBag$aggregate))
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
modFit$finalModel$prox)
modFit$finalModel$prox
training[,c(3,4)]
View(irisP)
pred <- predict(modFit,testing)
testing$predRight <- pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
library(ISLR);library(ggplot2); library(caret)
data(Wage)
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
predict(modFit,testing)
qplot(predict(modFit,testing),wage,data=testing)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-intrain,]
testing <- olive[-inTrain,]
library(caret)
modfit <- train(Area ~ ., method="rpart", data=training)
rm(modFit)
rm(modfit)
preObj <- preProcess(training, method="knnImpute")
rm(preObj)
modFit <- train(Area ~ .,data=training, preProcess="knnImpute", method="rpart")
rm(modFit)
preproc <- preProcess(training, method="knnImpute")
modFit <- train(Area ~., data=preproc, method="rpart")
modFit <- train(Area ~., data=training, method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata=newdata)
preProc <- as.data.frame(preproc$data)
modFit <- train(Area ~., data=preProc, method="rpart")
predict(modFit, newdata=newdata)
View(preProc)
View(training)
rm(preProc)
rm(preproc)
modFit <- train(Area ~., data=training, method="rpart")
summary(modFit)
View(olive)
as.factor(training$area)
9
training$Area <- as.factor(training$area)
olive$Area <- as.character(olive$Area)
View(olive)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
logRegMod <- glm(trainSA$chd ~ trainSA$age + trainSA$alcohol + trainSA$obesity + trainSA$tobacco +,family="binomial")
logRegMod <- glm(trainSA$chd ~ trainSA$age + trainSA$alcohol + trainSA$obesity + trainSA$tobacco +
trainSA$typea + trainSA$ldl, family="binomial")
logRegMod <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
logRegMod <- train(as.factor(chd) ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
logRegMod <- train(as.factor(chd, levels=2) ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
SAheart$chd <- as.factor(SAheart$chd)
View(SAheart)
library(ElemStatLearn)
data(SAheart)
head(SAheart)
View(SAheart)
SAheart$chd <- as.factor(SAheart$chd)
View(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
logRegMod <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass(testSA$chd, predict(modelSA, newdata = testSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(logRegMod, newdata = trainSA))
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1], size = dim(SAheart)[1] / 2, replace = F)
trainSA = SAheart[train, ]
testSA = SAheart[-train, ]
set.seed(13234)
modelSA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values, prediction){sum(((prediction > 0.5) * 1) != values) / length(values)}
missClass(trainSA$chd, predict(modelSA, newdata = trainSA))
missClass(testSA$chd, predict(modelSA, newdata = testSA))
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
View(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$case, p=0.7, list=FALSE)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Class ~ ., method="rpart", data=training)
print(modFit$finalModel)
library(rattle); library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
modFit <- train(Class ~ ., method="rpart", data=training)
print(modFit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
suppressMessages(library(caret))
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
modFit$finalModel
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.test)
head(vowel.train)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
modFit <- train(y ~ .,data=vowel.train,method="rf",prox=TRUE)
varImp(modFit)
library(ElemStatLearn); data(prostate)
str(prostate)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
str(vowel.test)
vowel$train$y <- as.factor(vowel.train$y)
voweltrain$y <- as.factor(vowel.train$y)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(caret)
rfFit <- train(y~ .,data=vowel.train,method="rf",prox=TRUE)
boostFit <- train(y ~ ., method="gbm",data=vowel.train,verbose=FALSE)
rfPred <- predict(rfFit,testing)
rfPred <- predict(rfFit,vowel.test)
boostPred <- predict(boostFit, vowel.test)
confusionMatrix(rfPred, vowel.test$y)
confusionMatrix(boostPred, vowel.test$y)
predDF <- data.frame(pred_rf, pred_gbm, y = vowel.test$y)
predDF <- data.frame(rfPred, boostPred, y = vowel.test$y)
View(predDF)
sum(rfPred[predDF$rfPred == predDF$boostPred] ==
predDF$y[predDF$rf == predDF$boostPred]) /
sum(predDF$rfPred == predDF$boost)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
rfFit <- train(diagnosis ~ .,data=training,method="rf",prox=TRUE)
boostFit <- train(diagnosis ~ ., method="gbm",data=training,verbose=FALSE)
ldaFit <- train(diagnosis ~ ., method="lda",data=training,verbose=FALSE)
Pred1 <- predict(rfFit, testing)
Pred2 <- predict(boostFit, testing)
Pred3 <- predict(ldaFit, testing)
PredDF <- data.frame(Pred1, Pred2, Pred3, diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~ ., method="gam", data=PredDF)
combPred <- predict(combModFit, PredDF)
View(PredDF)
confusionMatrix(combPred, PredDF$diagnosis)
confusionMatrix(Pred1, testing$diagnosis)
confusionMatrix(Pred2, testing$diagnosis)
confusionMatrix(Pred3, testing$diagnosis)
combModFit <- train(diagnosis ~ ., method="rf", data=PredDF)
combPred <- predict(combModFit, PredDF)
confusionMatrix(combPred, PredDF$diagnosis)
confusionMatrix(combPred, testing$diagnosis)$overall
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
modelFit <- train(CompressiveStrength ~ ., data=training, method="lasso")
?plot.enet
plot.enet(modelFit$finalModel,
xvar="penalty", use.color=TRUE)
modelFit$finalModel$beta.pure
set.seed(325)
install.packages("e1071")
library(e1071)
mod_svm <- svm(CompressiveStrength ~ ., data = training)
pred_svm <- predict(mod_svm, testing)
accuracy(pred_svm, testing$CompressiveStrength)
install.packages("forecast")
install.packages("quantmod")
library(forecast)
library(quantmod)
accuracy(pred_svm, testing$CompressiveStrength)
from.dat <- as.Date("01/01/2008", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("G00G", src="google", from=from.dat, to=to.dat)
head(G00G)
from.dat <- as.Date("01/01/2008", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from=from.dat, to=to.dat)
head(GOOG)
from.dat <- as.Date("01/01/08", format="%m/%d/%y")
to.dat <- as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
head(GOOG)
mGoog <- to.monthly(GOOG)
googOpen <- Op(mGoog)
ts1 <- ts(googOpen,frequency=12)
mGoog <- to.monthly(GOOG)
library(zoo)
mGoog <- to.monthly(GOOG)
library(xts)
mGoog <- to.monthly(GOOG)
library(lubridate)
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
library(lubridate)
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
mod_ts <- bats(tstrain)
fcast <- forecast(mod_ts, level = 95, h = dim(testing)[1])
sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper) /
dim(testing)[1]
setwd("~/Coursera/Practical Machine Learning/Course Project")
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
View(testing)
training$classe
View(testing)
str(training)
tail(training)
str(training$classe)
lifting <- read.csv("pml-training.csv")
library(caret)
inTrain <- createDataPartition(y=lifting$classe, p=0.6, list=FALSE)
training <- lifting[inTrain,]
testing <- lifting[-inTrain,]
nsv <- nearZeroVar(training, saveMetrics=TRUE)
nsv
View(nsv)
training <- training[,nsv$nzv=FALSE]
training <- training[,nsv$nzv="FALSE"]
training <- training[,nsv$nzv=="FALSE"]
colNames(training)
colnames(training)
testing <- testing[,nsv$nzv=="FALSE"]
Sums(is.na(training))
colSums(is.na(training))
colSums(is.na(testing))
training <- training[,colSums(is.na < 11526)]
training <- training[colSums(is.na(training)) < 11526]
testing <- testing[colSums(is.na(testing)) < 7688]
View(training)
training$row.names
training <- training[,7:59]
testing <- testing[,7:59]
cor(training)
library(Hmisc)
rcorr(training, type="pearson")
M <- abs(cor(training[,-53]))
diag(M) <- 0
which(M > .8, arr.ind=T)
which(M > .9, arr.ind=T)
M
preObj <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.9)
preObj
rm(preObj)
preObj90 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.9)
preObj80 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.8)
preObj80
preObj85 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.85)
preObj85
preObj80 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.8)
preObj85 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.85)
preObj90 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.9)
preObj80
preObj85
preObj90
PCs <- preProcess(log10(training[,-53]+1), method="pca", pcaComp=12)
PCs <- preProcess(training[,-53]), method="pca", pcaComp=12)
PCs <- preProcess(training[,-53], method="pca", pcaComp=12)
PCs <- preProcess(training[,-53], method="center", "scale", "pca", pcaComp=12)
PCs <- preProcess(training[,-53], method=c("center", "scale", "pca"), pcaComp=12)
trainPC <- predict(PCs, training[,-53])
modelFit_PCtree <- train(training$classe ~ .,method="rpart", data=trainPC)
library(rattle); library(rpart.plot)
fancyRpartPlot(modFit_PCtree$finalModel)
rm(modelFit_PCtree)
modFit_PCtree <- train(training$classe ~ .,method="rpart", data=trainPC)
fancyRpartPlot(modFit_PCtree$finalModel)
modFit_VarTree <- train(training$classe ~ .,method="rpart", data=training)
fancyRpartPlot(modFit_VarTree$finalModel)
PCtree <- fancyRpartPlot(modFit_PCtree$finalModel)
VarTree <- fancyRpartPlot(modFit_VarTree$finalModel)
grid.arrange(PCtree, VarTree, ncol=2)
library(gridExtra)
grid.arrange(PCtree, VarTree, ncol=2)
grid.arrange(PCtree, VarTree, ncol=2, top="PC Tree vs. Variable Tree")
PCtree
rm(PCtree)
rm(VarTree)
par(mfrow=c(1,2))
fancyRpartPlot(modFit_PCtree$finalModel, top="PC Tree")
fancyRpartPlot(modFit_PCtree$finalModel, main="PC Tree")
fancyRpartPlot(modFit_VarTree$finalModel, main="Variable Tree")
predictions_PCtree <- predict(modFit_PCtree, testing$classe)
predictions_VarTree <predict(modFit_VarTree, testing$classe)
predictions_VarTree <- predict(modFit_VarTree, testing$classe)
predictions_PCtree <- predict(modFit_PCtree, newdata=testing)
predictions_VarTree <- predict(modFit_VarTree, newdata=testing)
confusionMatrix(predictions_VarTree, testing$classe)
predictions_PCtree <- predict(modFit_PCtree, newdata=testing$classe)
modFit_PCtree <- train(training$classe ~ ., method= "rpart", preProcess="pca", data=training)
confusionMatrix(testing$classe, predict(modFit_PCtree, testing))
modFit_PCtree <- train(training$classe ~ ., method= "rpart", preProcess=c("center", "scale", "pca"), pcaComp=12, data=training)
library(caret)
lifting <- read.csv("pml-training.csv")
set.seed(1234)
inTrain <- createDataPartition(y=lifting$classe, p=0.6, list=FALSE)
training <- lifting[inTrain,]
testing <- lifting[-inTrain,]
nsv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nsv$nzv=="FALSE"]
testing <- testing[,nsv$nzv=="FALSE"]
colSums(is.na(training))
colSums(is.na(testing))
training <- training[colSums(is.na(training)) < 11526]
testing <- testing[colSums(is.na(testing)) < 7688]
training <- training[,7:59]
testing <- testing[,7:59]
M <- abs(cor(training[,-53]))
diag(M) <- 0
which(M > .9, arr.ind=T)
set.seed(1234)
preObj80 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.8)
preObj80
library(caret)
lifting <- read.csv("pml-training.csv")
set.seed(1234)
inTrain <- createDataPartition(y=lifting$classe, p=0.6, list=FALSE)
training <- lifting[inTrain,]
testing <- lifting[-inTrain,]
nsv <- nearZeroVar(training, saveMetrics=TRUE)
nsv
training <- training[,nsv$nzv=="FALSE"]
testing <- testing[,nsv$nzv=="FALSE"]
colSums(is.na(training))
colSums(is.na(testing))
training <- training[colSums(is.na(training)) < 11539]
testing <- testing[colSums(is.na(testing)) < 7675]
names(training)
names(testing)
training <- training[,7:59]
testing <- testing[,7:59]
M <- abs(cor(training[,-53]))
diag(M) <- 0
which(M > .9, arr.ind=T)
set.seed(1234)
preObj80 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.8)
preObj80
modFit_rf <- train(classe~ .,data=training,method="rf",prox=TRUE)
set.seed(1234)
modFit_rf <- train(classe~ .,data=training,method="rf",prox=TRUE)
warnings()
gc()
memory.limit()
memory.limit(size=1800)
