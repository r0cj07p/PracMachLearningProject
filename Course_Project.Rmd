---
title: Building a Machine Learning Algorithm to Predict the Manner of Exercise from
  Accelerometer Data
author: "Dave Moody"
date: "Thursday, May 12, 2016"
output:
  html_document:
    keep_md: yes
---
#Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data 
about personal activity relatively inexpensively. These type of devices are part of the quantified self movement, 
a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns 
in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a 
particular activity they do, but they rarely quantify how well they do it. 

For this project, 6 participants were asked to perform barbell lifts correctly and incorrectly in 5 different
ways, generating data from accelerometers positioned on the belt, forearm, arm, and dumbell of the participants. 

Using this data, the goal is to build a machine learning algorithm that predicts the manner in which the
participants performed an exercise, based on the measurements generated by the accelerometers during the exercise
.

#The Data
The data consists of 19642 observations of 160 different variables. Each observation is the execution of
a barbell lift by one of the participants.  The dataset is already divided into a training set and a testing 
set, with 20 of the observations allocated to the testing set to test the predictive algorithm at the conclusion 
of the predictive model building process.  

The outcome variable which the model will predict is the "classe" variable.  It is a factor variable with 5 
levels ("A", "B", "C", "D" and "E").  An "A" in the classe column indicates that the lift was performed correctly
, as determined by an experienced weight-lifter who supervised and directed the participants when performing the 
lifts.  Each lift was directed by the supervisor to be performed either correctly or with one or more specific 
movement errors (the participants used light weights to minimize or negate the risk of injury). Lifts performed 
with incorrect movements were assigned values "B" through "E" in the classe column, depending on the specific 
type of movement error performed.

The remaining variables in the dataset consist of the various measurement data collected by the accelerometers on 
the participants while they performed the lifts.  

The data, plus more information on Human Activity Recognition have been made available from the following website
: 
http://groupware.les.inf.puc-rio.br/har

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data 
Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence
. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, 
PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.

#Loading and Partitioning the Data
After downloading the data from the website above, the training dataset is read into Rstudio and partitioned into 
sub-training and testing sets.  The 20-observation testing set provided by the website will be used later as a 
validation set, and not loaded at this time or during any portion of the model building or testing phases. The 
following code assumes that the working directory is set to the file path containing the downloaded training set,
and that the name of the file was not changed.
```{r,message=FALSE}
library(caret)
lifting <- read.csv("pml-training.csv")
set.seed(1234)
inTrain <- createDataPartition(y=lifting$classe, p=0.6, list=FALSE)
training <- lifting[inTrain,]
testing <- lifting[-inTrain,]
```
Sixty percent of the observations in the lifting dataset are now allocated to a training set, and 40% are 
allocated to a testing set.  

#Preparing Data for Analysis
Prior to any model building, we will prepare the data for the analysis functions to follow. Missing values may 
need to be imputed, or some variables may need to be recategorized, combined or removed altogether depending on
their usefulness.  

###Removing Unnecessary Variables
If a variable in our datasets have zero or near-zero variability, it will serve no purpose in an analysis and 
should be removed from the datasets:
```{r}
nsv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nsv$nzv=="FALSE"]
testing <- testing[,nsv$nzv=="FALSE"]
```
This operation has trimmed the number of variables down from 160 to 105. 

A review of the remaining variables in the training set reveals that there are several columns where most of 
the values in the column are missing (NAs).  Interestingly, the columns identified as containing NAs appear to 
all contain the exact same number of NAs (11540 per column in the training set, constituting 97.9% of the 
observations, and 7676 per column in the testing set, also constituting 97.9% of the observations).  With only  
2.1% of the total observations in these columns containing data, these variables will be eliminated from the 
training and testing sets, as there are not enough values present in these columns to reasonably impute the 
missing ones, and these variables would have no predictive value in a model. 
```{r}
colSums(is.na(training))
colSums(is.na(testing))

training <- training[colSums(is.na(training)) < 11539]
testing <- testing[colSums(is.na(testing)) < 7675]
```
This operation has further trimmed the number of variables down from 105 to 59. According to the output of the 
"colSums" commands above, the remaining variables contain no missing values.

A review of the remaining variables reveals a small number of columns containing data that are not pertinent 
to our outcome variable of interest.  These include the data contained in the first 6 columns, which are 
observation/participant identifiers and timestamp data.  These will be removed manually.
```{r}
training <- training[,7:59]
testing <- testing[,7:59]
```
This operation has trimmed the number of remaining variables to 53. 

The final preprocessing step will be to examine the data for variables that are highly correlated with each 
other,and combine variables if necessary. We set a correlation coefficient threshold of .9, and identify the 
variables with correlation coefficients of .9 or higher:
```{r}
M <- abs(cor(training[,-53]))
diag(M) <- 0  #negate the instances of variables being perfectly correlated with themselves by changing their value to 0 
which(M > .9, arr.ind=T)
```
###Combining Variables?
The output above shows that several variables have a very high correleation with 2-3 others.  This list is even 
longer when the correlation coefficient threshold is lowered to .8, which is still a high correlation.  We may 
want to combine these highly correlated variables into single multivariables (via a principal component analysis) 
prior to building our predictive model, as this may effectively reduce the number of variables that the model 
has to consider while explaining the largest amount of variance in the data.  This could result in a more 
accurate model, but depending on how many multivariables we need in order to account for a large chunk of the 
variance, we may still end up with a large number of variables, which would defeat the purpose.  

To test this, we'll analyze how many principal components (PCs) we would need to account for 80% of the variance 
in the data:
```{r,message=FALSE}
set.seed(1234)
preObj80 <- preProcess(training[,-53], method=c("center", "scale", "pca"), thresh=0.8)
preObj80
```
According to the output above, we would need to create 12 PCs to account for 80% of the variance in the data.  
This indicates that it's not worthwhile to preprocess our variables in this manner, as it will hinder 
their interpretability by combining, centering and scaling the original variables into unrecognizable 
multivariables, and in the end, we would still be passing a lot of variables to the predictive models anyway, and 
risk overfitting the model.    
 
##Building a Predictive Model
Based on what was learned in the above review and cleaning of the dataset, a random forest method is being 
selected to build the predictive model.  The rationale for selecting this method is because it is well-suited to 
the classes of the predictors in our data (numeric and integer) and the outcome (factor). Random forests 
utililize classification trees to make predictions by placing cut points on numeric or integer data (which are
determined by analyzing the training data), then assigning the observations to different outcome nodes based on 
whether the value of the predictor variable is greater than or less than cut point. The variables that make the 
best predictors are selected by the model based on which ones produce the most homogenous groups, which also 
makes this method preferable since there are several dozen predictor variables in a detaset.

The random forest method also utilizes a built-in cross validation step in the form of resampling of both the 
observed data and the predictor variables at each level of the tree.  This way, each time the node of a tree is 
being evaluated, the method allows for a different subset of the predictor variables to potentially contribute to 
the next split, which is important since we have so many variables possibly contributing to 1 of 5 different 
outcomes in this case.
```{r,message=FALSE}
set.seed(1234)
library(randomForest)
modFit_rf <- randomForest(classe ~ .,data=training, importance=TRUE)
modFit_rf
```
The output of this model fit shows a very good performance when applied to the training data, according to the 
confusion matrix.  

Below is a table showing the 6 most important variables for determining the outcome by the model.  The variable 
importance measure here is based on weighted sums of the absolute regression coefficients. The weights are a 
function of the reduction of the sums of squares across the number of partial least squares components and are 
computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally 
to the reduction in the sums of squares. A "Max" column was added which identifies the highest weight value each
variable contributed, and the matrix is then ordered by the highest "Max" weight value, so looking at the top 6 rows of the newly ordered matrix shows the 6 variables with the most importance in determining outcome:
```{r,message=FALSE}
imp <- varImp(modFit_rf)
imp$Max <- apply(imp, 1, max)
impOrder <- imp[order(-imp$Max),]
head(impOrder)
```
#Estimating the Out of Sample Error
With the random forest method, there is no need for cross-validation or a separate test set to get an unbiased 
estimate of the test set error. It is estimated internally, during the run, and displayed in the output when 
the model is called.  For this model, the out of sample error rate estimate is only 0.66%, meaning we should 
expect this model to be > 99% accurate when applied to a distinct set of observations reserved for testing.

#Testing the Model on the Testing Sample
Now the random forest model we built will be applied to the testing data to predict outcomes on a new sample:
```{r}
set.seed(1234)
predictionsTestSet <- predict(modFit_rf,testing)
confusionMatrix(predictionsTestSet, testing$classe)
```
According to the confusion matrix above, the model predicted the outcomes in the testing data with over 99.2%
accuracy, with a 95% confidence interval of 99.07% on the low end.  With such a highly accurate result, this 
model is ready to be applied to the validation set.  The outcomes will be used to answer the quiz questions 
associated with this project.  

#Applying the model to the Validation Set
First, we have to load the validation dataset we downloaded previously into R, and clean it in the same manner as 
the training and test sets so that the predictor variables in each dataset are the same:
```{r}
ValSet <- read.csv("pml-testing.csv")
TrainColumns <- colnames(training[,-53]) #get the column names used in the training set
ValSet <- ValSet[TrainColumns] #subset the validation set, keeping only the same columns as the training set
```
Now we apply the random forest model to the validation set to predict the outcomes:
```{r}
set.seed(1234)
predictionsValSet <- predict(modFit_rf,ValSet)
predictionsValSet
```
The predictions above will now be submitted to complete the quiz.  
